{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f7158a7-a65f-4769-9df0-b0741c1594a6",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c5814-d496-48fe-9d83-333f539395c1",
   "metadata": {},
   "source": [
    "Why repeat analysis? When new members join a team or when members depart organizations, valuable information also leaves the organization due to jupyter Notebooks, word documents, or PDFs containing critical analysis existing on the departing members’ desktop. This issue of knowledge management causes many person-hours to be wasted. However, modern solutions allow analysis to persist beyond members' time at an organization. This project places this issue in the context of physics-aware machine learning of highly energetic material. It explicitly addresses data storage related to direct numerical simulations of a shock traveling through porous explosive material with a single void in its construction. When the shock occurs, chemical reactions cause the void to collapse and a denotation to occur, modeled using neural networks instead of direct numerical simulations. \n",
    "\n",
    "The specific goal of this project is to ingest the simulation dataset to present an analysis of how the state of the system changes over time before any machine learning. After reading the dataset, a dashboard will display the distribution of the fields of interest, elementary statistics (mean, median standard deviation) of the variables of interest, and more advanced analysis like a spatial gradient and an animation of the simulation. The current analysis of this dataset resides in a jupyter notebook with handpicked visualizations of a subset of the simulation data. The main contribution of this project is creating the free and open source implementation of an influx database for scientific machine learning concerning physics-informed machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fedbd9-5928-4f89-b1b2-0c2e7fd42b9f",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f775617-7562-4a7f-a9f6-d58d14db096b",
   "metadata": {},
   "source": [
    "### Physics Aware Recurrent Convolutional Neural Networks version 2\n",
    "This research uses Physics Aware Recurrent Convolutional Neural Networks version 2 (PARCv2) [1] to predict how the energetic material changes over time once a sock is applied to the system. This process can be described formally as an advection-diffusion-reaction system and is modeled using a partial differential equation (PDE) given below. \n",
    "$$ \\frac{\\partial \\textbf{x}}{\\partial t} = k  \\nabla \\textbf{x} - \\textbf{u} \\cdot \\Delta + \\textbf{R}_{\\textbf{x}}(\\textbf{x},\\textbf{u},\\textbf{c})$$\n",
    "\n",
    "\n",
    "With initial conditions,\n",
    "\n",
    "$$ \\textbf{u}(t=0) = \\textbf{u}_{0} $$\n",
    "$$ \\textbf{x}(t=0) = \\textbf{x}_{0} $$\n",
    "\n",
    "\n",
    "In the above PDE, $\\textbf{x}$ is the variable of interest: temperature, pressure, or microstructure at a position and time $ t$. $ k $ is the diffusivity coefficient. $ \\textbf{u} $ is the velocity field and is typically decomposed to $ \\textbf{u}_x $ and $\\textbf{u}_y$ to represent the velocity in the $x$ and $y$ direction, respectively.   If $ \\textbf{R}_{\\textbf{x}} $  is equal to zero, then it is known as Burgers’ equation. Essentially, PARCv2 is trying to learn the next state of the system, which can be solved numerically.[1]\n",
    "\n",
    "### Influx\n",
    "InfluxData Inc created InfluxDB is a NoSQL database focused on storing and visualizing time series data. It stores time series data in a parquet format and has an official docker image at https://hub.docker.com/_/influxdb. The utility it provides is the ability to query time series data quickly. [2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d54e6-5782-4e7f-bb6d-617c76f7d79d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3843735-ea17-49b6-92e4-973a9c0ac7db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The primary data source is 200 direct numerical simulations (DNS) of highly energic material receiving a shock over time. Each file contains velocity, temperature, and pressure fields at a $(x,y)$ position in space over time. Each simulation is a npy file representing a 128 by 256-pixel image with each $(x,y)$ pixel representing the location and containing temperature, pressure, and microstructure values at time $t$ of energetic material.\n",
    "\n",
    "Each simulation lasts for 20 to 40 nanoseconds. The temperature is measured in degrees kelvin and is valued in the range $[300, 5000]$. Velocity is measured in micrometers per nanosecond. Pressure is measured on gigapascals, and microstructure is expressed as in the range $[0,1]$. Each file is hosted in the Visual Intelligence Laboratory project folder on the University of Virginia High-Performance Computing HPC System, and in total, there are 30 GB of files.\n",
    "\n",
    " A fallback is using data at https://github.com/pdebench/PDEBench, an open-source repository of scientific machine-learning datasets. There is data for Burger's equation mentioned in the background, but it is a 93 GB file.[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d6296-8a70-4156-91cb-f2a93eef7f4b",
   "metadata": {},
   "source": [
    "## Potential Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147fbf3-f287-4ed1-b893-f2b6e469f150",
   "metadata": {},
   "source": [
    "In data science, one should understand the problem before making an inference. Therefore, the project aims to produce a dashboard to present exploratory analysis of the simulation data before feeding the data to the neural network within PARCv2. The project will focus on ingesting the npy files post-initial processing into a time series database and automatically updating a dashboard with key visualizations to enable the exploratory data analysis of the simulation data. The dashboard will have a dropdown list to select a particular simulation and then show the elementary statistics and the distribution of the variables of interest. Additionally, the dashboard will allow the user to see the simulation play over time. This is shown in the GIF below. The images shown is the temperature, pressure, microstructure, velocity in the x and velocity and the y direction changing over time once a shock is applied to the energiectic material.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5153b48-53ee-4c13-9633-0b0bebb3aecb",
   "metadata": {},
   "source": [
    "![title](images/00_002.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db840f00-55d1-4230-b005-d1e8b5ebe77e",
   "metadata": {},
   "source": [
    "The figure below shows an example histogram of the pressure data for every value in the pressure field of a 64 x 128 image over 25 timesteps from a simulation. For pressure alone, there are 204,800 values in just one simulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b72b3-b478-499e-960a-ce5edd54e8a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![title](images/pressure_histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa9049-a2d7-48e3-9ff0-4e5119d65b88",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f7aa73-7ea0-4dcd-856f-6c6051176d57",
   "metadata": {},
   "source": [
    "The main challenge of this project will be scope. Many of the example visualizations rely on the data having the shape of a tensor. Thus, adapting existing code to a new structure or designing the database queries to get the data in the correct format will be a challenge. Fortunately, there is utility in producing easily repeatable and transparent exploratory data analysis beyond this class project. Therefore, I plan to continue this work well beyond the fall semester. Handling the multi-dimensional arrays stored in the npy files is another challenge. The files must be unpacked and read into the influx db one channel at a time. Another issue is that Influx is a NoSQL database, and I have no experience with this type of backend. The syntax is written in Flux, Influx DB’s scripting and query. The documentation at https://docs.influxdata.com/influxdb/v2/ seems to be robust, but I will judge its quality once I dive in. Finally, the true challenge is reading from the data source outside the HPC environment. There are over 30 GB of files. However, I will only use a few files to develop locally. At a high level, the steps to success for this project are defining a database schema for the influx tables, writing the npy file to the influx database, querying the influx database to populate visualizations, and adapting existing visualizations to this new structure. Reading some of the influx documentation reveals that the npy files will potentially have to be converted to CSV’s to facilitate populating the database, which would also be challenging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e614fb7",
   "metadata": {},
   "source": [
    "# Project Check In 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb1eae",
   "metadata": {},
   "source": [
    "## Progress report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f80726",
   "metadata": {},
   "source": [
    "summarizes the project and all of the progress made since the beginning of the semester. <br><br>\n",
    "I have confirmed that the data is available under MIT License [4]. They are stored as npy files. Each file has the shape (timesteps, channels, height, width), where the number of channels is five, and the height and weight of the image is 128 x 256 pixels. The figure Interactive Visualization Mock-up is an example of the interactive plot that will be placed on the dashboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e0eae4",
   "metadata": {},
   "source": [
    "### Interactive Visualization Mock up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8188aef8",
   "metadata": {},
   "source": [
    "![title](images/pressure_demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47015f9c",
   "metadata": {},
   "source": [
    "Data wrangling involved looping through each array channel and storing the timestep, height, and width in a data frame. The data frame is then converted to JSON for reading into the database—an example of the dataframe after data wrangling is given below."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ff58a54",
   "metadata": {},
   "source": [
    "timestep\tx\ty\tvalue\n",
    "0\t0\t0\t0\t3.942014e+08\n",
    "1\t0\t0\t1\t-1.200000e-05\n",
    "2\t0\t0\t2\t-1.200000e-05\n",
    "3\t0\t0\t3\t-1.200000e-05\n",
    "4\t0\t0\t4\t-1.200000e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ab8ee",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Each dataframe will have on the order of five hundred thousand records. This is value was obtained using 15 as an example number of timestep and multiplying timesteps, height and width."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911cd86",
   "metadata": {},
   "source": [
    "## Data summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654062d8",
   "metadata": {},
   "source": [
    "In plain language, describes your choices with regard to selecting features to keep from the raw data, the wrangling needed, and the number and types of databases chosen to store the data. <br><br>\n",
    "\n",
    "\n",
    "Initially, influxDB was chosen as the backend database. The parameters necessary to make a connection to the database are as follows:\n",
    "\n",
    "influx_password = os.getenv('DOCKER_INFLUXDB_INIT_PASSWORD') <br>\n",
    "influx_user= os.getenv('DOCKER_INFLUXDB_INIT_USERNAME') <br>\n",
    "influx_token= os.getenv('DOCKER_INFLUXDB_INIT_TOKEN') <br>\n",
    "influx_org= os.getenv('DOCKER_INFLUXDB_INIT_ORG') <br>\n",
    "influx_bucket= os.getenv('DOCKER_INFLUXDB_INIT_BUCKET') <br>\n",
    "\n",
    "client = influxdb_client.InfluxDBClient(url=\"http://localhost:8086\", username=influx_user, password=influx_password, token=influx_token, org=influx_org) <br><br>\n",
    "I am currenttly having an issue with writing my data to the influxdb. The write example  below from the influx documentation works but when I adapt it to my data, nothing gets writen in the measurement bucket.\n",
    "\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS) <br>\n",
    "\n",
    "p = influxdb_client.Point(\"my_measurement\").tag(\"location\", \"Prague\").field(\"temperature\", 25.3)<br>\n",
    "write_api.write(bucket=influx_bucket, org=influx_org, record=p)<br>\n",
    "\n",
    "Due to this issue, MongoDB will store the data. As stated in the previous section, each npy file has been converted to a dataframe to enable the creation of the dashboard's data visualizations. This data frame is then converted to JSON and subsequently read into the MongoDB. Five documents, one for each channel, are created for one simulation file. The structure of the json is as follows:\n",
    "\n",
    "{ <br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;\"variable\": \"pressure\",  <br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;\"file\": \"void_100\",  <br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;\"Data\": [  <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{\"timestep\": 0, \"x\": 0, \"y\": 0, \"value\": 100.0},  <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{\"timestep\": 0, \"x\": 0, \"y\": 1, \"value\": 200.0},  <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{\"timestep\": 0, \"x\": 0, \"y\": 2, \"value\": 300.0}  <br>\n",
    "  ]} <br>\n",
    "\n",
    "In the example json, variables can be temperature, pressure, microstructure, velocity (x) or velocity (y). The file key is the name of the npy file. The data key contains the information within the npy file. <br><br>\n",
    "After successfuly creating the dashboard using the mongo database, experimentation with the influx database will continue as time allows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc64d5c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Since this is a NoSql database the are no any relationships to document in an ERD. However, an example diagram of the mongoDB structure of each file is available at https://dbdocs.io/clarencew0083/parc_dash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179a36a",
   "metadata": {},
   "source": [
    "As stated in the data section, each simulation lasts for 20 to 40 nanoseconds. The temperature is measured in degrees kelvin and is valued in the range $[300, 5000]$. Velocity is measured in micrometers per nanosecond. Pressure is measured on gigapascals, and microstructure is expressed as in the range $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e40f0e",
   "metadata": {},
   "source": [
    "# Database documentation and visualizations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0926d2a2",
   "metadata": {},
   "source": [
    "the first draft of complete database documentation for each database you build for this project. The documentation for one database should include tables that list the name of every feature in the database, its data type, whether the feature is a primary key or a foreign key that links to a primary key in another table, and short notes that describe what the feature means. The documentation should also include an ER diagram if thereare multiple tables included in the database. You may construct all of this by hand, or you can use an automated tool such as https://dbdocs.io."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5636bfe-7f88-4219-b68b-d40e54a3de22",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3168da-2d7b-4a13-950d-1ee8dfb1c960",
   "metadata": {},
   "source": [
    "\n",
    "[1] Nguyen, et al., “PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling,” arXiv (Cornell University), Feb. 2024, doi: https://doi.org/10.48550/arxiv.2402.12503.\n",
    "‌\n",
    "\n",
    "[2] “InfluxDB | Real-time insights at any scale | InfluxData,” InfluxData, Sep. 23, 2024. https://www.influxdata.com/homepage/ (accessed Oct. 08, 2024).\n",
    "\n",
    "‌[3] Takamoto, Makoto; Praditia, Timothy; Leiteritz, Raphael; MacKinlay, Dan; Alesiani, Francesco; Pflüger, Dirk; Niepert, Mathias, 2022, \"PDEBench Datasets\", https://doi.org/10.18419/darus-2986, DaRUS, V8\n",
    "\n",
    "[4] Visual Intelligence Laboratory. (2024). PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling [Data set]. Zenodo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63319cf2-3dbd-480b-99c3-4f2fe57766cc",
   "metadata": {},
   "source": [
    "## Gen AI Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c581ac-4ae1-448e-914f-270a7b74dada",
   "metadata": {},
   "source": [
    "I used generative AI to assist with data wrangling and converting existing visualizations to plotly. Additionally, I used the spell check and editor feature of Grammarly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd52a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
